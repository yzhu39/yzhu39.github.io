<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yi Zhu @ UB</title>

    <meta name="language" content="en-US">

    <link href="./Yi_Zhu_homepage_files/style.css" rel="stylesheet" type="text/css">

</head>

<body>
    <div id="container">
        <div id="wrapper">
            <div id="info">
                <img style=" float:left; margin-top:5px;
				-webkit-border-radius: 10px;
				-moz-border-radius: 10px;
				border-radius: 10px;
				margin-right:20px;
				" src="./Yi_Zhu_homepage_files/test.jpg" width="183px" height="200px;">

                <h2 style="margin-top:15px; margin-bottom:5px; color:#000;"> <b>Yi Zhu</b></h2>
                <p>
                    <a style="color:#000; text-decoration: none;"> Department of Computer Science and Engineering </a>
                    <br>
                    <a style="color:#000; text-decoration: none;"> University at Buffalo, State Univesrity of New York</a>
                    <br>
                    <a style="color:#000; text-decoration: none;"> Buffalo, NY 14228 </a>
                </p>
                <br>

                <p style="color:#000;">
                    <b>Office:</b> 301C Davis Hall <br>
                    <b>Email:</b> yzhu39 at buffalo dot edu <br>
                </p>
            </div>

            <div id="content">

                <div id="bio">
                    <h2 style="margin-top:26px; color:#000">Biography</h2>
                    <br>
                    <p>
                        I am currently a PhD candidate in the <a href="https://engineering.buffalo.edu/computer-science-engineering.html">Department of Computer Science and Engineering</a> at <a href="https://www.buffalo.edu/">University at Buffalo, State University of New York</a>, supervised by Prof. <a href="https://cse.buffalo.edu/~qiao/index.htm">Chunming Qiao</a> and Prof. <a href="https://engineering.purdue.edu/~lusu/">Lu Su</a>. 
                        <br>
                        My research interests lie in the areas of security, autonomous driving, and machine learning. I am especially interested in studying the vulnerability of autonomous driving systems to malicious attacks and developing novel AI techniques to make them robust and reliable.
                    </p>

                </div>

                <div id="research">
                    <h2 style="margin-top:26px; color:#000;">Research</h2>
                    <br>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    
                    <tr>
                        <td style="vertical-align:top">
                            <img src="./Yi_Zhu_homepage_files/test.jpg" width="170px" height="auto" style="float:left; margin-right:3%; ">
                        </td>
                        <td style="vertical-align:top">
                            <strong>[CCS'21]</strong>
                            "Can we use arbitrary objects to attack LiDAR perception in autonomous driving?",
                            <em>the 2021 ACM SIGSAC Conference on Computer and Communications Security</em>, Virtual Conference, November 2021. <br>
                            <script>
                            function toggleMultiAbs() {
                                var text = document.getElementById("multi_abs");
                                if (text.style.display === "none") {
                                    text.style.display = "block";
                                } else {
                                    text.style.display = "none";
                                }
                            }
                            </script>
                            <a href="." onclick="toggleMultiAbs(); return false;">[Abstract]</a>
                            <a href="Yi_Zhu_homepage_files/papers/CCS21_434.pdf"> [PDF]</a>
                            <a href="https://youtu.be/S-QOpFCfgwU"> [Demo]</a>
                            <p id="multi_abs" style="display: none">
                                As an effective way to acquire accurate information about the driving environment, LiDAR perception has been widely adopted in autonomous driving. The state-of-the-art LiDAR perception systems mainly rely on deep neural networks (DNNs) to achieve good performance. However, DNNs have been demonstrated vulnerable to adversarial attacks. Although there are a few works that study adversarial attacks against LiDAR perception systems, these attacks have some limitations in feasibility, flexibility, and stealthiness when being performed in real-world scenarios. In this paper, we investigate an easier way to perform effective adversarial attacks with high flexibility and good stealthiness against LiDAR perception in autonomous driving. Specifically, we propose a novel attack framework based on which the attacker can identify a few adversarial locations in the physical space. By placing arbitrary objects with reflective surface around these locations, the attacker can easily fool the LiDAR perception systems. Extensive experiments are conducted to evaluate the performance of the proposed attack, and the results show that our proposed attack can achieve more than 90% success rate. In addition, our real-world study demonstrates that the proposed attack can be easily performed using only two commercial drones. To the best of our knowledge, this paper presents the first study on the effect of adversarial locations on LiDAR perception models’ behaviors, the first investigation on how to attack LiDAR perception systems using arbitrary objects with reflective surface, and the first attack against LiDAR perception systems using commercial drones in physical world. Potential defense strategies are also discussed to mitigate the proposed attacks.
                            </p> 
                        </td>
                    </tr> 

                    <tr>
                        <td style="vertical-align:top">
                            <img src="./Yi_Zhu_homepage_files/test.jpg" width="170px" height="auto" style="float:left; margin-right:3%; ">
                        </td>
                        <td style="vertical-align:top">
                            <strong>[CCS'21]</strong>
                            "Can we use arbitrary objects to attack LiDAR perception in autonomous driving?",
                            <em>the 2021 ACM SIGSAC Conference on Computer and Communications Security</em>, Virtual Conference, November 2021. <br>
                            <script>
                            function toggleMultiAbs() {
                                var text = document.getElementById("multi_abs");
                                if (text.style.display === "none") {
                                    text.style.display = "block";
                                } else {
                                    text.style.display = "none";
                                }
                            }
                            </script>
                            <a href="." onclick="toggleMultiAbs(); return false;">[Abstract]</a>
                            <a href="Yi_Zhu_homepage_files/papers/CCS21_434.pdf"> [PDF]</a>
                            <a href="https://youtu.be/S-QOpFCfgwU"> [Demo]</a>
                            <p id="multi_abs" style="display: none">
                                As an effective way to acquire accurate information about the driving environment, LiDAR perception has been widely adopted in autonomous driving. The state-of-the-art LiDAR perception systems mainly rely on deep neural networks (DNNs) to achieve good performance. However, DNNs have been demonstrated vulnerable to adversarial attacks. Although there are a few works that study adversarial attacks against LiDAR perception systems, these attacks have some limitations in feasibility, flexibility, and stealthiness when being performed in real-world scenarios. In this paper, we investigate an easier way to perform effective adversarial attacks with high flexibility and good stealthiness against LiDAR perception in autonomous driving. Specifically, we propose a novel attack framework based on which the attacker can identify a few adversarial locations in the physical space. By placing arbitrary objects with reflective surface around these locations, the attacker can easily fool the LiDAR perception systems. Extensive experiments are conducted to evaluate the performance of the proposed attack, and the results show that our proposed attack can achieve more than 90% success rate. In addition, our real-world study demonstrates that the proposed attack can be easily performed using only two commercial drones. To the best of our knowledge, this paper presents the first study on the effect of adversarial locations on LiDAR perception models’ behaviors, the first investigation on how to attack LiDAR perception systems using arbitrary objects with reflective surface, and the first attack against LiDAR perception systems using commercial drones in physical world. Potential defense strategies are also discussed to mitigate the proposed attacks.
                            </p> 
                        </td>
                    </tr> 

                    </tbody></table>
                </div>

                <div id="pro_act">
                    <h2 style="margin-top:26px; color:#000">Professional Service</h2>
                    <h4 style="margin-top:0px;">Journal Reviewer</h4>
                    <li>ACM Transactions on Sensor Networks (TOSN)</li>

                    <h4 style="margin-top:0px;">Conference External Reviewer</h4>
                    <li>ACM International Conference on Mobile Computing and Networking (MobiCom 2022)</li>
                    <li>IEEE International Conference on Distributed Computing Systems (ICDCS 2022)</li>
                    <li>ACM International Conference on Mobile Computing and Networking (MobiCom 2021)</li>
                    <li>ACM Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc 2021)</li>
                    <li>IEEE International Conference on Computer Communications (INFOCOM 2021)</li>
                    <li>ACM International Conference on Mobile Computing and Networking (MobiCom 2020)</li>

                    <h4 style="margin-top:0px;">Program Committee</h4>
                    <li>ACM SenSys+BuildSys Workshop on Data: Acquisition to Analysis (DATA 2022)</li>
                </div>

                <div id="teaching">
                    <h2 style="margin-top:26px; color:#000">Teaching Experiences</h2>
                    <h4 style="margin-top:0px;">Teaching Assistant</h4>
                    <ul>
                        <li>Spring 2021, Modern Network Concepts</li>
                        <li>Fall 2020, Introduction to Data Mining</li>
                        <li>Spring 2020, Computer Organization</li>
                        <li>Fall 2019, Introduction to Programming for Engineers</li>
                        <li>Spring 2019, Electrical Engineering Concepts for Non-Majors</li>
                        <li>Fall 2018, Real-Time and Embedded Operating Systems</li>
                    </ul>
                </div>

                <div id="honor">
                    <h2 style="margin-top:26px; color:#000">Honors and Rewards</h2>
                    <br>
                    <ul>
                        <li>2018-2022, Presidential Fellowships, University at Buffalo, the State University of New York</li>
                        <li>2021     , CSE Best Graduate Research Award, University at Buffalo, the State University of New York</li>
                    </ul>
                </div>

            </div>
        </div>
    </div>

</body></html>